\chapter{Introduction}

Growing systems is a concern for large
organisations.~\cite[p.~1]{stoel_storm_vinju_bosman_2016} The continuity of
systems becomes difficult and a single modification can result in unexpected
behaviour of a larger part of the system.

Within the domain knowledge, reasoning about the expected behaviour of a system,
changes and errors are hard. \textit{Rebel} aims to solve these challenges by centralising the domain knowledge and relating
it to the running systems. \textit{Rebel} is a formal specification language to
control the intrinsic complexity of software for financial
enterprise systems.~\cite[p.~1]{stoel_storm_vinju_bosman_2016}

Software testing is an important part of software projects.~\cite[p.~4]{myers2011art} The testing
process within large systems can be challenging, it entails not only defining
and executing many test cases, solving thousands of errors, handling
thousands of modules, but also enormous project management. To facilitate this
process \textit{Rebel} offers automated simulation and checking of specifications with
the use of a \gls{smt} solver. This solves to some
extent the testing and reasoning of \textit{Rebel} specifications, but this is only
within in the \textit{Rebel} domain.

Code generators generate code from the \textit{Rebel} specifications. The problem with code generation is that the resulting
product is leaving the \textit{Rebel} domain, causing loss of testing and reasoning with
the use of formal methods. The challenge is to regain the benefits from the
\textit{Rebel} domain to be able to test and reason about running systems.
% The challenge is to regain the benefits from the \textit{Rebel} domain to be able to test and reason about systems with the use of formal methods.

\section{Problem statement}\label{sec:problem-statement}

% Ultimately, running systems should be generated from \textit{Rebel} specifications. Since \textit{Rebel} is a declarative language it will not always be straightforward to generate a correct system from this. Again SMT solvers might hold the key as shown in other work like [9].

According to the study~\cite[p.~3]{stoel2015case}, it should be possible to generate
running systems from \textit{Rebel} specifications. Right now this is possible, and
running systems are generated from \textit{Rebel} specifications. It is not always straightforward to generate a correct system from \textit{Rebel}
specifications since \textit{Rebel} is a declarative language.~\cite[p.~3]{stoel2015case}
As mentioned before, the simulation and checking for the correctness of
specifications is only in the \textit{Rebel} domain.

The running systems which are
generated from specifications need to be properly based on these specifications,
it should be conform to these specifications. So additional work is necessary for
the generation process to know that running systems are conform to the
specifications.

The language \textit{Rebel} promised to be deterministic, this also holds
for the generated system. Thus, non-deterministic behaviour in the generated
system should be identified.

Especially for ING Bank, it is important that there is no corrupted data within
the runtime systems.

\subsection{Solution direction}\label{sec:solution-direction}

The research is about testing the implementation correctness of specifications.
For the problems in \autoref{sec:problem-statement}, the
study~\cite[p.3]{stoel2015case} proposed a possible solution for these problems,
which is to use \gls{smt} solvers. As before mentioned, the mapping of the \textit{Rebel}
language to the \gls{smt} formulas makes it possible to check and simulate
specifications. As a result of this, there is an interpreter for \textit{Rebel}
specifications, which is the \gls{smt}
solver.~\cite[p.5]{stoel_storm_vinju_bosman_2016}

In the same study,
an attempt of model-based testing is done to test real banking systems.
According to the study, it is only possible to test interactively using the
simulation. The steps made in the simulation are executed in the \gls{sut}, any
differences in behaviour are displayed in the simulator. The future work of this
approach is to expand the functionality to work automatically with a given
trace.

Due to all these reasons, it seems to be a good solution to use the \gls{smt}
solver which holds the key in testing the generated system. Theoretically, with
this approach, it is possible to regain the benefits from the \textit{Rebel} domain, and
again able to test and reason about \textit{Rebel} specifications and generated system.

\subsubsection{Expectations}

The main research question is as follows: \textit{How to validate the generated code
from a Rebel specification?}. To research this, the \gls{smt} solver is used as
an oracle for testing the generated code. So the \gls{smt} solver will be used
to test the implementation correctness of specifications in the generated
system. To clarify implementation correctness, we emphasise templating,
compilation and distribution. This applies to the code generators and the
generated code. The implementation correctness dimensions can also be found in
the experiments. These are therefore discussed in detail in these chapters.

These are dimensions that can cause faults in the implementation. A fault can
be introduced by templating or compilation or distribution errors.
The expectation is to find the first faults in the first two dimensions,
templating or compilation since faults in distribution are more difficult to
find. The three implementation correctness dimensions are discussed below:
\begin{itemize}
  \item \textbf{Templating} The code generators use templating to generate code
  from the specifications. The generated code should correctly map to the input
  code from templates. If not, the generated code and \textit{Rebel}
  specifications will have different meanings.
  \item \textbf{Compilation} The generated code from the code generators needs
  to compile. Otherwise, it is not possible to run or test the generated system.
  \item \textbf{Distribution} The implementation of the generated system must
  conform to the \textit{Rebel} semantics, \textit{e.g.}, synchronisation and
  distribution. For instance, transition atomicity should also be guaranteed in
  the generated systems. A transition is only allowed to be executed when the
  preconditions hold. As part of postconditions, no transitions should change
  the relevant values before the preconditions and during the execution of the
  transition. After the execution of the transition, the postconditions of the
  transition should hold.
  Concepts such as transactions~\cite[p.~6]{tanenbaum2017distributed} and
  locking~\cite[p.~10]{tanenbaum2017distributed} influence transition atomicity
  in the implementation of the specifications (generated system).
\end{itemize}

\subsubsection{Assumptions}

With the given approach, a few assumptions need to be made:
\begin{itemize}
\item \textbf{The specification is always correct.}
The specifications are written correctly, \textit{i.e.}, the specifications are
correctly modelled from the business point of view.
It is not effective to test incorrect specifications. Testing
inconsistent behaviour is senseless and therefore wasted time. It is also much
more difficult because there is nothing to tell about the expectations.
\item \textbf{The generated system can be compiled.} Note that we are testing
the generated code, not the generator. Showing that a generator always generates
compilable code is a different interesting questions which is out of the scope
of this thesis.
\item \textbf{The \textit{Rebel} specifications are correctly interpreted by the \gls{smt}
solver.} The \gls{smt} solver is used as an oracle/black box in the testing approach
since it is an interpreter for \textit{Rebel} specification. However, when
something goes wrong with the mapping of the \textit{Rebel} language to the \gls{smt}
formulas, this will result into misbehaviour of the specification which may lead
to incorrect results.
\end{itemize}

\subsection{Research questions}\label{sec:research-questions}
The following questions are defined to achieve the research goal:

\begin{description}
  \item [RQ] How to validate the generated code from a Rebel specification?

  \begin{description}
    \item [SQ1] How is the input/output of the generated system tested?
    \item [SQ2] Which false positives occur when the generated
    system is correctly implemented?
    \item [SQ3] What kind of faults can be found and what are the factors?
  \end{description}

\end{description}

\subsection{Research method}\label{sec:research-method}
We test generated systems by the code generators in two ways, invalid execution
and valid execution. The first experiment tests invalid execution in the
generated systems. Therefore, the test framework will use checking to check the
satisfiability of a given specification. However, testing valid execution can
also provide valuable results. The second experiment tests valid execution in
the generated systems with the use of checking and simulation.

At first, an initial lightweight version will be developed; then it will be extended with motivated
improvements with evaluation and validation. The proof of concept is a testing
tool for testing the implementation correctness of a specification of \gls{sut}.

The
approach is to start with the lightweight version which can trigger a fault and
test it with the \gls{smt} solver. A fault is seen as the deviation between the
current behaviour and the expected behaviour.~\cite{delgado2004taxonomy, leucker2009brief}
Typically this is identified by the deviation between the current and expected
state of the system. In our case, the expected state and behaviour is defined in
the \textit{Rebel} specifications.

For the lightweight version, it is an easily
reproducible fault. Then the lightweight version is improved with smarter
testing techniques to generate tests automatically, and these improvements are done
with evaluation and validation. For example, by using existing software testing
techniques like Concolic testing~\cite{sen2007concolic},
Fuzz testing~\cite{godefroid2008automated} and Mutation testing~\cite{jia2011analysis}.

\section{Contributions}
The research has the following contributions:

\begin{enumerate}
  \item Methodologies to validate generated systems from \textit{Rebel}
  specifications. These methodologies include an in-depth analysis and
  evaluation of the results.

  \item Limitations in \textit{Rebel} and \gls{smt} encoding as this an
  important part of the test approach. These limitations can lead to false
  positives when the generated system is generated correctly.

  \item The faults and factors encountered in the generated system that was found
  using the methodologies.
\end{enumerate}

\section{Related Work}

Testing generated systems can be performed on different aspects. This section
briefly introduces relevant work of this thesis.

\subsection*{Model-based testing}

Model-based testing entails the process and techniques for automatic generation
of test cases using abstract models.~\cite{utting2012taxonomy, tretmans2008model, dalal1999model}
Test cases are generated based on these models and then executed on the
\gls{sut}. These models represent the behaviours of a \gls{sut} and/or its
environment.~\cite{utting2012taxonomy, tretmans2008model}

After defining the model, test selection criteria need to be defined to
transform these criteria into test case specifications. Test case specifications
describe on a high level the desired test case. Test cases are generated when
the model and test case specifications are defined.~\cite{utting2012taxonomy}
Then a test execution environment can be used to automatically execute test
cases and record verdicts.

The main difference with our approach and model-based testing is that the model
is already present. The model in \textit{Rebel} is the \textit{Rebel}
specifications. \textit{Rebel} specifications describe banking products, and
also running systems are generated from it. The model in model-based testing is
built from informal requirements or existing specification
documents.~\cite[p.~2]{utting2012taxonomy} This model
shares the same characteristics as \textit{Rebel} specifications.

In model-based testing, there exist several test generation technologies to
generate test cases, such as random generation, (bounded) model checking,
etc.~\cite[p.~8-9]{utting2012taxonomy}
As mentioned earlier, \textit{Rebel} offers automated simulation and checking of
specifications with the use of an \gls{smt} solver. For both simulation and
checking, \textit{Rebel} uses bounded model checking. Our approach is also using
the bounded model checking to test the \gls{sut}.

\subsection*{Runtime verification}

Runtime verification is a technique to ensure that a system at the time of
execution meets the desired
behaviour.~\cite{leucker2009brief, havelund2008verify, falcone2009runtime}
Runtime verification is seen as a lightweight verification in addition to
verification techniques like model checking and
testing.~\cite[p.~294]{leucker2009brief} This gives the possibility to react
when misbehaviour of a system is detected. The origins of runtime verification
are in model checking, but a variant of linear temporal logic is often used.
The main difference between runtime verification and other verification
techniques is that runtime verification is performed at runtime.
The focus of runtime verification is to detect satisfactions or violations of
safety properties.~\cite{leucker2009brief, falcone2009runtime}

A so-called monitor in runtime verification performs the checking whether an
execution in the system meets the safety property.~\cite[p.~295]{leucker2009brief}
The device which reads a finite trace and gives a certain verdict is called
monitor. The monitors are usually automatically generated from a high
specification in runtime verification.~\cite{leucker2009brief, falcone2009runtime}

The main similarity of runtime verification with our approach is the ability to
test systems at runtime. In our approach, the generated systems are being tested
against the \gls{api}, which is at runtime.
Runtime verification is only considered to detect satisfactions or violations of
safety properties.~\cite{leucker2009brief, falcone2009runtime}
In our approach, simulation and checking, which uses bounded model checking,
will be used to test the generated systems. Bounded model checking is also used
to check whether a safety property holds.~\cite[p.~4]{stoel_storm_vinju_bosman_2016}
Some property of interest which is used in bounded model checking to check
whether it holds is called a safety property. Although, the approach we have
chosen for is not only to check the safety property but also to check whether a
certain execution is (not) possible in the generated system.

The main difference between runtime verification and model checking is the
presence of a model of the system to be checked.
Runtime verification refers only to executions observed as they are generated by
the real system; thus there is no system model.~\cite[p.~295]{leucker2009brief}
However, with model checking, a model of the system to be checked needs to be
build to check all possible executions.

As said before, in runtime verification are the monitors usually automatically
generated from a high specification in runtime verification. In comparison to
our approach, we are not going to generate monitors since we are going to test
the generated systems with simulation and checking,
whether the generated systems behaves conform to the specification.

\subsection*{Property-based testing}

Property-based testing is a software testing approach where the generic
structure of valid inputs of the program needs to be defined combined with
properties which are expected to hold for every valid
input.~\cite[p.~3]{papadakis2011proper} The properties relate to the behaviour
of the program and the input-output relationship. Using these data, a
property-based testing tool can automatically generate randomly valid input.
This input is then applied to the program while monitoring the execution to
test whether it behaves as expected. A well-known property-based testing tool is
QuickCheck~\cite{claessen2011quickcheck} for Haskell.

Property is a partial high-level specification of the \gls{sut}. In comparison
to full specification, properties are compact and easy to write and
understand.~\cite[p.~3]{papadakis2011proper} For example, a property could be
for a given method which takes a list as an argument, the returned list from
this method must have the same size as the passed list. This property must hold
despite the passed list. Like this, properties can be specified for
\textit{Rebel} or a \textit{Rebel} specification.

At the same time of this research, another master's student has also researched
the testing of generated systems from \textit{Rebel} specifications.
This approach uses property-based testing.~\cite{kok2017property}
There are three main differences between this and our approach.

Firstly, the property-based testing approach uses one \textit{Rebel}
specification to test the generated system. The defined properties in this
\textit{Rebel} specification should also hold in the generated environment.
These properties should hold in the generated environment despite the defined
\textit{Rebel} specifications because these properties are bound to the
\textit{Rebel} domain semantics. In our approach, we can use any \textit{Rebel}
specification to test the generated system whether it behaves according to the
specification. Our approach is less tied to the \textit{Rebel} semantics but
places more emphasis on the defined specification.

Secondly, the property-based testing approach uses offline testing, and our
approach uses online testing. The property-based testing approach generated unit
tests based on the defined properties.
With offline testing~\cite{utting2012taxonomy} test cases are generated strictly
before they are run. This is also the case with property-based testing approach
since we know in advance which test cases are generated.
In our testing approach, we got two online systems, namely the \gls{smt} solver
and the generated system. The \gls{smt} solver has in our approach an important
part since it is used to generate test cases. We do not know in advance which
test cases are being generated by the \gls{smt} solver.

Thirdly, test execution is in our approach done at runtime.
As mentioned earlier, the property-based testing approach generated unit tests.
These unit tests are run in the test mode of the generated system.

\subsection*{Testing distributed systems}

Distributed systems are difficult to build, because of partial failure and
asynchrony.~\cite[p.~1]{mccaffrey2016verification}
In order to create a correct system, these two concepts of distributed systems
need to be addressed. Solving these problems often result in complex solutions.
The study~\cite{mccaffrey2016verification} describes verifications of
distributed systems in two categories, namely formal verification and
verification in the wild.

Formal verification is a systematic process which uses mathematical reasoning to
proves properties about a system.~\cite{mccaffrey2016verification, sanghavi2010formal}
This results in a system that can be said to be provably correct.
Formal specification languages allow to model and verify the correctness of
concurrent systems.~\cite[p.~2]{mccaffrey2016verification}
An example of a formal specification language is TLA+ which is used with by
\gls{aws} to verify critical systems.~\cite[p.~1]{newcombe2014use}
\gls{aws} has applied various techniques (fault-injection testing, stress testing,
etc.). However, they still found subtle bugs hidden in complex fault-tolerant
systems.
The use of TLA+ has yielded valuable results for \gls{aws} in two ways. Finding
bugs that could not be found in other ways, and making performance optimisations
without the loss of correctness.~\cite[p.~3]{newcombe2014use}

Model checking, which is also a formal method, determines whether a system is
provably correct. Model checkers systematically use state-space exploration to
provide the paths for a system.~\cite[p.~3]{mccaffrey2016verification}
A system is provably correct when all path have been executed.
\gls{aws} has used TLA+ specifications in combination with a model checker. This
resulted in the identification of a bug that could cause data loss in DynamoDB,
which is a data store.~\cite[p.~7]{newcombe2014use}
The shortest trace for this bug contained 35 steps.

Compared to formal verification, given that formal verification is expensive,
test methods can be used that give confidence that the systems are built
correctly.~\cite[p.~4]{mccaffrey2016verification}
Simple test methods such as unit and integration tests or property-based testing
can already be a way to test distributed systems.
Fault-injection testing is a test method for causing or introducing an error in
the system. By forcing the occurrence of failures, it allows the observation
and measurement of the systems by the engineers.

The main similarity with our approach and the approach described above is the
use of formal verification. \textit{Rebel} is also formal specification
language.~\cite[p.~1]{stoel_storm_vinju_bosman_2016} Model checking is also
available with \textit{Rebel} specifications, although this is bounded. Other
test methods as described above can be used to test distributed systems, but
this is out of the scope of this thesis.

\section{Outline}
This section outlines the structure of the thesis. \autoref{sec:ch2} contains
the background of this thesis. As this research focuses on experiments that
validate generated systems, each experiment is divided into its own chapter.
The experiments test the generated systems in two ways, invalid execution and
valid execution. The lightweight version which tests invalid execution is
discussed in \autoref{sec:ch3}. The invalid execution experiment and its results
are discussed in \autoref{sec:ch4}. The valid execution and its results are
discussed in \autoref{sec:ch5}. \autoref{sec:ch6} contains the answers for each
research question, also containing a discussion of the conducted experiments,
limitations and found faults. Finally, a conclusion of this thesis is given in
\autoref{sec:ch7}.
