\chapter{Discussion}

In this chapter, we discuss further the results from the experiments and answers
will be given to the sub research questions.

\section{RQ 1: How is the input/output of the generated system tested?}

\subsection{Experiment 1: Invalid execution traces}
In \autoref{sec:ch4}, we have seen an experiment where it is possible to automatically generate a test for every transition from a given specification. This experiment tests the opposite of a specification, i.e. testing what should be not possible according to the specification. Testing is done by using checking for a given transition and testing this in the \gls{sut}.

Testing the opposite is also often used in mutation testing. Mutation testing generates faulty programs (mutants) by syntactic changes, in our case, we create only one faulty version of the program. The faulty program is generated based on the mutation operator Negate Conditionals Mutator. The mutant in this testing approach is killed when the result from the \gls{smt} solver and the \gls{sut} are the same.

In \autoref{sec:ch4-evaluation}, we discussed the evaluation of this experiment. In short, this experiment produces some false positives/negatives. The reasons for this are varying results from the \gls{smt} solver and the comparability of performed transitions between the \gls{smt} solver and the \gls{sut}. Also, the \gls{smt} solver is smarter and better in checking the checking the satisfiability. With this experiment, we did find two bugs in the \gls{sut}; only one bug is within our scope.
% The theory for this found bug is $\forall e s_{1} \to s_{2}, s_{2} \gets s_{1}, pre(e)$.

In this experiment are traces not used since with checking traces are not
available when a state is not reachable. The assumption was that these states
were not reachable due to the opposite preconditions. As discussed in the
experiment, in some transitions the state to reach are reachable, \textit{e.g.},
when no preconditions are supplied or when a precondition is not applied on a
property of a specification (which is not part of a transition). In the
experiment is mutation testing applied on the executed transition in the
\gls{sut}. Instead of mutation testing the executed transition in the
\gls{sut}, mutation testing can also be applied to the \textit{Rebel}
specifications.
The specifications are then mutated, then tests can be generated that
distinguish between the original model and the mutated
specification.~\cite[p.~8]{utting2012taxonomy}
Traces are also available with this approach since only the specification is
mutated and it can be interpreted by the \gls{smt} solver. These traces can be
used to test the \gls{sut}, which allows this approach to be combined with the
second experiment. As a result, the limitation of this experiment is solved
with the use of \gls{smt} solver; it is not necessary anymore to check the
satisfiability in the \gls{sut}. The study \cite{paradkar2005case} reports that
model-based testing technique using mutation has valuable fault detection
effectiveness.

% [1,9] related to mutation model-based testing

\subsection{Experiment 2: Valid execution traces}
The experiment of \autoref{sec:ch5} has solved a few limitation of the previous experiment. This experiment extends the model testing approach with Rebel.

In this model testing approach are the traces used to check whether the \gls{sut} accept the execution from the trace and whether it behaves as the specification. Therefore are the traces used in this approach.

In comparison to the previous experiment, this approach tests what should be possible according to the specification. Even the transition parameters data values are generated by the \gls{smt} solver which satisfies the constraints of the transition. This experiment uses two existing testing techniques within \textit{Rebel} to generate tests for transitions, namely checking and simulation.

To perform and test transitions from traces are three steps done, pre-transition check, transition check and post-transition check. Also, with this testing approach, we did found bugs in the code generators. The intention is to test all transitions, but with this testing approach, it is only possible to test transitions when the preconditions of a given transition are satisfied by the current state. The theory for this testing approach is as follows: $\forall e s_{1} \to s_{2}, (! s_{2} pre(e) \lor s_{2} pre(e) in s_{1} post(e))$.

\section{RQ 2: Are there any false positives/negatives when the generated system has been implemented correctly?}

\subsection{Varying results from the SMT solver}
In \autoref{sec:ch4-evaluation}, we already discussed the limitation of this testing approach. The test run of this experiment produces some false positives/negatives. As mentioned earlier, this is due to the varying results from the \gls{smt} solver and the comparability of performed transitions between the \gls{smt} solver and the \gls{sut}.

% future work mutating the specification

\subsection{Invalid current state}
In \autoref{sec:close-no-test-codegenakka}, we have seen that the experiment is not able to test the \textit{close} transition. In short, it is not able to test this transition because the current state and its values were not satisfying for the transition to the next state. In this case, the simulation is not able to perform the transition, although this transition can be made in the \gls{sut} with satisfying parameters.

The current state for the transitions is generated by the test framework. It is possible to generate current states based on the conditions of the chosen transition, but this can become complex when multiple complex specifications are used. Again, this is playing the \gls{smt} solver; the \gls{smt} solver is better/smarter in doing this kind of computations. So it would be better to extend the model checker and define conditions of transitions. This is left as future work.

To conclude, this experiment is only able to test transitions when the preconditions of a given transition are satisfied by the current state.

% future work model checker

\subsection{Identifiers for entities}
We discussed in \autoref{sec:ch5-current-state} that the identifiers for generating the current state are generated by the test framework. This is done for the following reasons, the uniqueness of the identifiers are only within a trace and the limitation of the \gls{smt} formulas of \textit{Rebel} types.

In the case of the type \textit{IBAN}, the given identifier by the \gls{smt} solver is auto-incremented, only unique in one trace and not compliant with the ISO\_13616 and ISO\_9362 standards. Thus the \textit{Rebel} types are interpreted by the \gls{smt} solver are not conform to the \textit{Rebel} types. This can cause problems when these values from the type are read from a given trace and tested against the \gls{sut}. Therefore, is a random generator implemented, for only \textit{IBAN} and \textit{Integer}, which generates appropriate values which can be used to test against the \gls{sut}.

On the other hand, due to the misinterpreted \textit{Rebel} types, it is possible that \gls{smt} solver is not able to solve a given specification.

\info{what are the definitions of Rebel types?}

\section{RQ 3: What kind of bugs can be found and what are the factors?}

From the conducted experiments and found bugs, we can categorise these bugs as follows: compilation, templating and distribution.

\subsection{Compilation}
In the category compilation belongs bugs which are not compilable systems generated by the code generators from a given specification. For this category, we did find a bug which is discussed in \autoref{sec:bug-compile-max-deposit}. Just as mentioned there, this bug is out of scope, since the intention is to find misbehaviour in \gls{sut} with our test framework.

\subsection{Templating}
Most of the bugs we did find with the experiments are within the category templating. We split this category into two parts, fixed code and injected code. With the experiment from \autoref{sec:ch5}, we did find two bugs in the code generators. One bug is from the Javadatomic generator, the code which causes the bug is not generated from the specification. This code is part of the skeleton where the code from the specification is generated. So, the identified bug in the \gls{sut} belongs to the category templating within the fixed code. The bug within the Scala-ES generator belongs to injected code. The identified bug is caused by the generation of code from a given specification. To conclude, within the category templating, it is possible to have bugs either in fixed code or injected code.

\subsection{Distribution}
Distribution is another category for bugs, although we did not find any bugs related to distribution with our experiments. The experiment from \autoref{sec:ch5} tests also transitions which contain synchronisation. Since no bugs are found, it can be assumed that the synchronised transitions works, in the sense of the result of the tested transitions were the same as the trace from the \gls{smt} solver. As discussed before, the synchronised transitions are also translated to logical formulas. These formulas contain no logic about synchronisation, and therefore it should be possible to identify misbehaviour in synchronisation. Several studies also claim to find faults in distributed algorithms.

% encoding strategy.
