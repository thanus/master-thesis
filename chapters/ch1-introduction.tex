\chapter{Introduction}

\epigraph{Discovering the unexpected is more important than confirming the
known.}{George E. P. Box}

Growing systems is a concern for large
organisations.~\cite[p.~1]{stoel_storm_vinju_bosman_2016} The continuity of
systems becomes difficult and a single modification can result in unexpected
behaviour of a larger part of the system.

Within the domain knowledge, reasoning about the expected behaviour of a system,
changes and errors are hard. \textit{Rebel} aims to solve these challenges by centralising the domain knowledge and relating
it to the running systems. \textit{Rebel} is a formal specification language to
control the intrinsic complexity of software for financial
enterprise systems.~\cite[p.~1]{stoel_storm_vinju_bosman_2016}

Software testing is an important part of software projects.~\cite[p.~4]{myers2011art} The testing
process within large systems can be challenging, it entails not only defining
and executing many test cases, solving thousands of errors, handling
thousands of modules, but also enormous project management. To facilitate this
process \textit{Rebel} offers automated simulation and checking of specifications with
the use of an \gls{smt} solver. This solves to some
extent the testing and reasoning of \textit{Rebel} specifications, but this is only
within in the \textit{Rebel} domain.

Code generators generate code from the \textit{Rebel} specifications. The problem with code generation is that the resulting
product is leaving the \textit{Rebel} domain, causing loss of testing and reasoning with
the use of formal methods. The challenge is to regain the benefits from the
\textit{Rebel} domain to be able to test and reason about running systems.
% The challenge is to regain the benefits from the \textit{Rebel} domain to be able to test and reason about systems with the use of formal methods.

\section{Problem statement}\label{sec:problem-statement}

% Ultimately, running systems should be generated from \textit{Rebel} specifications. Since \textit{Rebel} is a declarative language it will not always be straightforward to generate a correct system from this. Again SMT solvers might hold the key as shown in other work like [9].

According to the study~\cite[p.~3]{stoel2015case}, it should be possible to generate
running systems from \textit{Rebel} specifications. Right now this is possible, and
running systems are generated from \textit{Rebel} specifications. It is not always straightforward to generate a correct system from \textit{Rebel}
specifications since \textit{Rebel} is a declarative language.~\cite[p.~3]{stoel2015case}
As mentioned before, the simulation and checking for the correctness of
specifications is only in the \textit{Rebel} domain.

The running systems which are
generated from specifications need to be properly based on these specifications,
it should be conform to these specifications. So additional work is necessary for
the generation process to know that running systems are conform to the
specifications.

The language \textit{Rebel} promised to be deterministic, this also holds
for the generated system. Thus, non-deterministic behaviour in the generated
system should be identified.

Especially for ING Bank, it is important that there is no corrupted data within
the runtime systems.

\subsection{Research questions}\label{sec:research-questions}
The following questions are defined to achieve the research goal:

\begin{description}
  \item [RQ] How to validate the generated code from a Rebel specification?

  \begin{description}
    \item [SQ1] How is the input/output of the generated system tested?
    \item [SQ2] Are there any false positives/negatives when the generated
    system has been implemented correctly?
    \item [SQ3] What kind of bugs can be found and what are the factors?
  \end{description}

\end{description}

\subsection{Research method}\label{sec:research-method}

The research is about testing the implementation correctness of specifications.
For the problems in \autoref{sec:problem-statement}, the
study~\cite[p.3]{stoel2015case} proposed a possible solution for these problems,
which is to use \gls{smt} solvers. As before mentioned, the mapping of the \textit{Rebel}
language to the \gls{smt} formulas makes it possible to check and simulate
specifications. As a result of this, there is an interpreter for \textit{Rebel}
specifications, which is the \gls{smt}
solver.~\cite[p.5]{stoel_storm_vinju_bosman_2016}

In the same study,
an attempt of model-based testing is done to test real banking systems.
According to the study, it is only possible to test interactively using the
simulation. The steps made in the simulation are executed in the \gls{sut}, any
differences in behaviour are displayed in the simulator. The future work of this
approach is to expand the functionality to work automatically with a given
trace.

Due to all these reasons, it seems to be a good solution to use the \gls{smt}
solver which holds the key in testing the generated system. Theoretically, with
this approach, it is possible to regain the benefits from the \textit{Rebel} domain, and
again able to test and reason about \textit{Rebel} specifications and generated system.

The hypothesis is that the \gls{smt} solver can be used to test the implementation
correctness of a specification against a generated system. To do this,
research needs to be carried out for testing the generated system
with the \gls{smt} solver, mappings (traces) between these systems. With the given approach, a
few assumptions need to be made:

\begin{itemize}
\item \textbf{The specification is always correct.}
The specifications are written correctly, \textit{i.e.}, the specifications are
correctly modelled from the business point of view. An incorrect specification
will probably not pass the checking or simulation, and testing a generated
system derived from an incorrect specification is not really effective.
\item \textbf{The generated system can be compiled.} When the generated
system cannot be compiled, it is possible that there might be a fault in the
code generator. Although, with this approach testing a not compilable system is
impossible.
\item \textbf{The \textit{Rebel} specifications are correctly interpreted by the \gls{smt}
solver.} The \gls{smt} solver is used as an oracle/black box in the testing approach
since it is an interpreter for \textit{Rebel} specification. However, when
something goes wrong with the mapping of the \textit{Rebel} language to the \gls{smt}
formulas, this will result into misbehaviour of the specification which may lead
to incorrect results.
\end{itemize}

At first, an initial lightweight version is expected, then it will be extended with motivated
improvements with evaluation and validation. The proof of concept is a testing
tool for testing the implementation correctness of a specification of \gls{sut}.

The
approach is to start with the lightweight version which can trigger a bug and
test it with the \gls{smt} solver. For the lightweight version, it is an easily
reproducible bug. Then the lightweight version is improved with smarter
testing techniques to generate tests automatically, and these improvements are done
with evaluation and validation. For example, by using existing software testing
techniques like Concolic testing~\cite{sen2007concolic}, Fuzz testing~\cite{godefroid2008automated} and Mutation testing~\cite{jia2011analysis}.

\section{Contributions}
% The research has the following contributions:
%
% \begin{enumerate}
%   \item Tool for testing a generated system from a specification.
%   \item Limitations in \textit{Rebel} or SMT encoding.
%   \item Bugs and its categories.
%   \item Solved the future work of \textit{Rebel} paper
% \end{enumerate}

\section{Related Work}

\subsection*{Model-based testing}

Model-based testing entails the process and techniques for automatic generation
of test cases using abstract models.~\cite{utting2012taxonomy, tretmans2008model, dalal1999model}
Test cases are generated based on these models and then executed on the
\gls{sut}. These models represent the behaviours of a \gls{sut} and/or its
environment.~\cite{utting2012taxonomy, tretmans2008model}

After defining the model, test selection criteria need to be defined to
transform these criteria into test case specifications. Test case specifications
describe on a high level the desired test case. Test cases are generated when
the model and test case specifications are defined.~\cite{utting2012taxonomy}
Then a test execution environment can be used to automatically execute test
cases and record verdicts.

The main difference with our approach and model-based testing is that the model
is already present. The model in \textit{Rebel} is the \textit{Rebel}
specifications. \textit{Rebel} specifications describe banking products, and
also running systems are generated from it. The model in model-based testing is
built from informal requirements or existing specification
documents.~\cite[p.~2]{utting2012taxonomy} This model
shares the same characteristics as \textit{Rebel} specifications.

In model-based testing, there exist several test generation technologies to
generate test cases, such as random generation, (bounded) model checking,
etc.~\cite[p.~8-9]{utting2012taxonomy}
As mentioned earlier, \textit{Rebel} offers automated simulation and checking of
specifications with the use of an \gls{smt} solver. For both simulation and
checking, \textit{Rebel} uses bounded model checking. Our approach is also using
the bounded model checking to test the \gls{sut}.

\subsection*{Runtime verification}

Runtime verification is seen as a lightweight verification in addition to
verification techniques like model checking and
testing.~\cite[p.~294]{leucker2009brief} This gives the possibility to react
when misbehaviour of a system is detected. The origins of runtime verification
are in model checking, but a variant of linear temporal logic is often used.
The main difference between runtime verification and other verification
techniques is that runtime verification is performed at runtime.
The focus of runtime verification is to detect satisfactions or violations of
safety properties.~\cite[p.~294]{leucker2009brief}

The monitor performs the checking whether an execution in the system meets the
safety property.~\cite[p.~295]{leucker2009brief}
The device which reads a finite trace and gives a certain verdict is called
monitor. The monitors are usually automatically generated from a high
specification in runtime verification.

The main similarity of runtime verification with our approach is the ability to
test systems at runtime. In our approach, the generated systems are being tested
against the \gls{api}, which is at runtime.
Runtime verification is only considered to detect satisfactions or violations of
safety properties.~\cite[p.~298]{leucker2009brief}
In our approach, simulation and checking, which uses bounded model checking,
will be used to test the generated systems. Bounded model checking is also used
to check whether a safety property holds.~\cite[p.~4]{stoel_storm_vinju_bosman_2016}
Although, the approach we have chosen for is
not only to check the safety property but also to check whether a certain
execution is (not) possible in the generated system.

The main difference between runtime verification and model checking is the
presence of a model of the system to be checked.
Runtime verification refers only to executions observed as they are generated by
the real system; thus there is no system model.~\cite[p.~295]{leucker2009brief}
However, with model checking, a model of the system to be checked needs to be
build to check all possible executions.

As said before, in runtime verification are the monitors usually automatically
generated from a high specification in runtime verification. In comparison to
our approach, we are not going to generate monitors since we are going to test
the generated systems with simulation and checking,
whether the generated systems behaves conform to the specification.

% \subsection*{Testing generated systems / test generation}

\section{Outline}
The outline of this thesis.
