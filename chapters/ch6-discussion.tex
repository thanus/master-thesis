\chapter{Discussion}

In this chapter, we discuss further the results from the experiments and answers will be given to the sub research questions. Hereafter, threats to the validity of the conducted experiments are discussed.

\section{RQ 1: How is the input/output of the generated system tested?}

\subsection{Experiment: Mutated Checking}
In \autoref{sec:ch4}, we've seen a proof of concept where it is possible to automatically generate a test for every transition from a given specification. This proof of concept test the inverse of a specification, i.e. testing what should be not possible according to the specification. Testing is done by using checking for a given transition and testing this in the SUT.

Testing the inverse is also often used in mutation testing. Mutation testing generates faulty programs (mutants) by syntactic changes, in our case, we create only one faulty version of the program. The faulty program is generated based on the mutation operator Negate Conditionals Mutator. The mutant in this testing approach is killed when the result from the SMT Solver and the SUT are the same.

In \autoref{sec:ch4-evaluation}, we discussed the evaluation of this proof of concept. In short, this experiment produces some false positives/negatives. The reasons for this are varying results from the SMT Solver and the comparability of performed transitions between the SMT Solver and the SUT. Also, the SMT Solver is smarter and better in checking the checking the satisfiability. With this proof of concept, we did find two bugs in the SUT, only one bug is within our scope.
% The theory for this found bug is $\forall e s_{1} \to s_{2}, s_{2} \gets s_{1}, pre(e)$.


\subsection{Experiment: Checking & Simulation}
The proof of concept of \autoref{sec:ch5} has solved a few limitation of the previous proof of concept. This proof of concept extends the model testing approach with Rebel.

In this model testing approach are the traces used to check whether the SUT accept the execution from the trace and whether it behaves as the specification. Therefore are the traces used in this approach.

In comparison to the previous proof of concept, this approach tests what should be possible according to the specification. Even the transition parameters data values are generated by the SMT Solver which satisfies the constraints of the transition. This proof of concept uses two existing testing techniques within Rebel to generate tests for transitions, namely checking and simulation.

In order to perform and test transitions from traces are three steps done, pre-transition check, transition check and post-transition check. Also, with this testing approach, we did found bugs in the code generators. The intention is to test all transitions, but with this testing approach, it is only possible to test transitions when the preconditions of a given transition are satisfied by the current state. The theory for this testing approach is as follows: $\forall e s_{1} \to s_{2}, (! s_{2} pre(e) \lor s_{2} pre(e) in s_{1} post(e))$.

\section{RQ 2: Are there any false positives/negatives when the generated system has been implemented correctly?}

\subsection{Varying results from the SMT solver}
In \autoref{sec:ch4-evaluation}, we already discussed the limitation of this testing approach. The test run of this proof of concept produces some false positives/negatives. As mentioned earlier, this is due to the varying results from the SMT Solver and the comparability of performed transitions between the SMT Solver and the SUT.

% future work mutating the specification

\subsection{Invalid current state}
In \autoref{sec:close-no-test-codegenakka}, we've seen that the proof of concept isn't able to test the \textit{close} transition. In short, it isn't able to test this transition because the current state and its values were not satisfying for the transition to the next state. In this case, the simulation isn't able to perform the transition, although this transition can be made in the SUT with satisfying parameters.

The current state for the transitions is generated by the test framework. It is possible to generate current states based on the conditions of the chosen transition, but this can become complex when multiple complex specifications are used. Again, this is playing the SMT Solver, the SMT Solver is better/smarter in doing this kind of computations. So it would be better to extend the model checker and define conditions of transitions. This is left as future work.

To conclude, this proof of concept is only able to test transitions when the preconditions of a given transition are satisfied by the current state.

% future work model checker

\subsection{Identifiers for entities}
We discussed in \autoref{sec:ch5-current-state} that the identifiers for generating the current state are generated by the test framework. This is done for the following reasons, the uniqueness of the identifiers are only within a trace and the limitation of the SMT formulas of Rebel types.

In the case of the type \textit{IBAN}, the given identifier by the SMT Solver is auto-incremented, only unique in one trace and not compliant with the ISO\_13616 and ISO\_9362 standards. Thus the \textit{Rebel} types are interpreted by the SMT Solver aren't conform to the \textit{Rebel} types. This can cause problems when these values from the type are read from a given trace and tested against the generated system. Therefore, is a random generator implemented, for only \textit{IBAN} and \textit{Integer}, which generates appropriate values which can be used to test against the SUT.

On the other hand, due to the misinterpreted \textit{Rebel} types, it is possible that SMT Solver isn't able to solve a given specification.

\info{what are the definitions of Rebel types?}

\section{RQ 3: What kind of bugs can be found and what are the factors?}

From the conducted proof of concepts and found bugs, we can categorise these bugs as follows: compilation, templating and distribution.

\subsection{Compulation}
In the category compilation belongs bugs which are not compilable systems generated by the code generators from a given specification. For this category, we did find a bug which is discussed in \autoref{sec:bug-compile-max-deposit}. Just as mentioned there, this bug is out of scope, since the intention is to find misbehaviour in running systems with our test framework.

\subsection{Templating}
Most of the bugs we did find with out proof of concepts are within the category templating. We split this category into two parts, fixed code and injected code. With the proof of concept from \autoref{sec:ch5}, we did find two bugs in the code generators. One bug is from the Javadatomic generator, the code which causes the bug isn't generated from the specification. This code is part of the skeleton where the code from the specification is generated. So, the identified bug in the SUT belongs to the category templating within fixed code. The bug within the Scala-ES generator belongs to injected code. The identified bug is caused by the generation of code from a given specification. To conclude, within the category templating, it is possible to have bugs either in fixed code or injected code.

\subsection{Distribution}
Distribution is another category for bugs, although we didn't find any bugs related to distribution with our proof of concepts. The proof of concept from \autoref{sec:ch5} tests also transitions which contain synchronisation. Since no bugs are found, it can be assumed that the synchronised transitions works, in the sense of the result of the tested transitions were the same as the trace from the SMT Solver. As discussed before, the synchronised transitions are also translated to logical formulas. These formulas contain no logic about synchronisation, and therefore it should be possible to identify misbehaviour in synchronisation. Several studies claim also to find faults in distributed algorithms.

% encoding strategy.
