\chapter{Discussion}\label{sec:ch6}

In this chapter, we discuss further the results from the experiments and answers
will be given to the sub research questions.

\section{RQ 1: How is the input/output of the generated system tested?}

\subsection{Experiment 1: Invalid execution}
In \autoref{sec:ch4}, we have seen an experiment where it is possible to
automatically generate a test for every transition from a given specification.
This experiment tests the opposite of a specification, i.e. testing what should
be not possible according to the specification. Testing is done by using
checking for a given transition and testing this in the \gls{sut}.

Testing the opposite is also often used in mutation testing. Mutation testing
generates faulty programs (mutants) by syntactic changes, in our case, we create
only one faulty version of the program. The faulty program is generated based on
the mutation operator Negate Conditionals Mutator. The mutant in this testing
approach is killed when the result from the \gls{smt} solver and the \gls{sut}
are the same.

In \autoref{sec:ch4-evaluation}, we discussed the evaluation of this experiment.
In short, this experiment produces some false positives/negatives. The reasons
for this are varying results from the \gls{smt} solver and the comparability of
performed transitions between the \gls{smt} solver and the \gls{sut}. Also, the
\gls{smt} solver is smarter and better in checking the checking the
satisfiability. With this experiment, we did find two faults in the \gls{sut};
only one fault is within our scope. The found faults belong to the fault categories
templating and compilation.

In this experiment are traces not used since with checking traces are not
available when a state is not reachable. The assumption was that these states
were not reachable due to the opposite preconditions. As discussed in the
experiment, in some transitions the state to reach are reachable, \textit{e.g.},
when no preconditions are supplied or when a precondition is not applied on a
property of a specification (which is not part of a transition). In the
experiment is mutation testing applied on the executed transition in the
\gls{sut}. Instead of mutation testing the executed transition in the
\gls{sut}, mutation testing can also be applied to the \textit{Rebel}
specifications.
The specifications are then mutated, then tests can be generated that
distinguish between the original model and the mutated
specification.~\cite[p.~8]{utting2012taxonomy}
Traces are also available with this approach since only the specification is
mutated and it can be interpreted by the \gls{smt} solver. These traces can be
used to test the \gls{sut}, which allows this approach to be combined with the
second experiment. As a result, the limitation of this experiment is solved
with the use of \gls{smt} solver; it is not necessary anymore to check the
satisfiability in the \gls{sut}. The study \cite{paradkar2005case} reports that
model-based testing technique using mutation has valuable fault detection
effectiveness.

% [1,9] related to mutation model-based testing

\subsection{Experiment 2: Valid execution}
The experiment of \autoref{sec:ch5} has solved a few limitation of the previous
experiment. This experiment extends the model-based testing approach which was
already done with \textit{Rebel}.

This experiment tests valid execution in the generated systems. In comparison to
the previous experiment, this approach tests what should be possible according
to the specification. This experiment uses two existing testing techniques
within \textit{Rebel} to generate tests for transitions, namely checking and
simulation. It uses the traces from the \gls{smt} solver to check whether the
\gls{sut} accept the execution from the traces and whether it behaves as the
specification. Even the transition parameters data values are generated by the
\gls{smt} solver which satisfies the constraints of the transition.

In this experiment, the transitions are performed in the \gls{sut} in three
steps to identify misbehaviour in the \gls{sut}. The execution of the transition
contains the followings three steps: pre-transition check, transition check and
post-transition check. These three steps are used to check that the execution
from the \gls{sut} matches to the traces of specifications.

More complex specifications are used in this experiment which implements
synchronisation. The test framework has successfully tested all generators which
implements the synchronisation. No faults were found, except in the distributed
Codegen-Akka. In the distributed Codegen-Akka faults were found in not unanimous
final outcome between the nodes. The nodes of the \gls{sut} are not consistent
since each node may produce a different result. So with this test approach
faults can be found in distributed systems.

Although there is a limitation in the testing of states. Within a trace, not
every instance may contain the state of it (inconclusive state).
The intention is to test all transitions,
but with this testing approach, it is only possible to test
transitions when the preconditions of a given transition are satisfied by the
current state. Thus theory for this testing approach is as follows:
$\forall e s_{1} \to s_{2}, (! s_{2} pre(e) \lor s_{2} pre(e) in s_{1} post(e))$.

All in all, three faults are found with this experiment in the generated systems
and code generators. One fault is identified by a slightly different test approach
to find distribution faults. The found faults belong to the fault categories
templating and distribution.

\section{RQ 2: Are there any false positives/negatives when the generated system has been implemented correctly?}

\subsection{Varying results from the SMT solver}
In \autoref{sec:ch4-evaluation}, we already discussed the limitation of this
testing approach. The test run of this experiment produces some false
positives/negatives. As mentioned earlier, this is due to the varying results
from the \gls{smt} solver and the comparability of performed transitions between
the \gls{smt} solver and the \gls{sut}.

\subsection{Invalid current state}
In \autoref{sec:close-no-test-codegenakka}, we have seen that the experiment is
not able to test the \textit{close} transition. In short, it is not able to test
this transition because the current state and its values were not satisfying for
the transition to the next state. In this case, the simulation is not able to
perform the transition, although this transition can be made in the \gls{sut}
with satisfying parameters.

The current state for the transitions is generated by the test framework. It is
possible to generate current states based on the conditions of the chosen
transition, but this can become complex when multiple complex specifications are
used. Again, this is playing the \gls{smt} solver; the \gls{smt} solver is
better/smarter in doing this kind of computations. So it would be better to
extend the model checker and define conditions of transitions. This is left as
future work.

To conclude, this experiment is only able to test transitions when the
preconditions of a given transition are satisfied by the current state.

% future work model checker

\subsection{Identifiers for entities}
We discussed in \autoref{sec:ch5-current-state} that the identifiers for
generating the current state are generated by the test framework. This is done
for the following reasons, the uniqueness of the identifiers are only within a
trace and the limitation of the \gls{smt} formulas of \textit{Rebel} types.

In the case of the type \textit{IBAN}, the given identifier by the \gls{smt}
solver is auto-incremented, only unique in one trace and not compliant with the
ISO\_13616 and ISO\_9362 standards. Thus the \textit{Rebel} types are
interpreted by the \gls{smt} solver are not conform to the \textit{Rebel} types.
This can cause problems when these values from the type are read from a given
trace and tested against the \gls{sut}. Therefore, a random generator is
implemented, for only \textit{IBAN} and \textit{Integer}, which generates
appropriate values which can be used to test against the \gls{sut}.

On the other hand, due to the misinterpreted \textit{Rebel} types, it is
possible that \gls{smt} solver is not able to solve a given specification.

\section{RQ 3: What kind of faults can be found and what are the factors?}

From the conducted experiments, five faults have been found in the
generated system that is generated by the code generators. We can categorise
these faults in the following categories: templating, compilation and
distribution.

\subsection{Templating}
The code generators use templating to generate code from \textit{Rebel}
specifications. The generated code should correctly map to the input code from
templates. Otherwise, the generated code and \textit{Rebel} specifications will
have different meanings.

Most of the faults we did find with the experiments are within the category
templating. We split this category into two parts, fixed code and injected code.
Fixed code is definitive code that is part of generated systems and will not be
changed regardless of the \textit{Rebel} specifications.
Injected code is code which is generated from \textit{Rebel} specifications that
fills some isolated parts of the fixed code that are dependent on the input of
the specifications.

With the experiment from \autoref{sec:ch5}, we did find two templating faults in the code
generators. One fault is from the Javadatomic generator, the code which causes the
fault is not generated from the specification. This code is part of the fixed code
where the code from the specification is generated. So, the identified fault in
the \gls{sut} belongs to the category templating within the fixed code.
The fault within the Scala-ES generator belongs to injected code. The identified fault is
caused by the generation of code from a given specification.

To conclude, within
the category templating, it is possible to have faults either in fixed code or
injected code.

\subsection{Compilation}
In the category compilation belongs faults which are not compilable systems
generated by the code generators from a given specification. For this category,
we did find a fault which is discussed in \autoref{sec:bug-compile-max-deposit}.
Just as mentioned there, this fault is out of scope, since the intention is to
find misbehaviour in \gls{sut} with our test framework.
Therefore, the generated system from the code generators needs to compile.

\subsection{Distribution}
Distribution is another category for faults, although we did not find any faults
related to distribution with our experiments when the \gls{sut} is run as single
nodes.

The implementation of the generated system must conform to the
\textit{Rebel} semantics, \textit{e.g.}, synchronisation and distribution. For
instance, transition atomicity should also be guaranteed in the generated
systems. Concepts such as transactions~\cite[p.~6]{tanenbaum2017distributed} and
locking~\cite[p.~10]{tanenbaum2017distributed} influence transition atomicity in
the implementation of the specifications (generated system). Partial failure
and asynchrony makes it also difficult to build and test distributed
systems.~\cite[p.~1]{mccaffrey2016verification}

In the experiment from \autoref{sec:ch5} faults were found in
distribution when the Codegen-Akka system is run in a distributed mode. In the
distributed Codegen-Akka faults were found in not unanimous final outcome
between the nodes. There is no consistency between the nodes since each node may
produce a different result.

The experiment from \autoref{sec:ch5} tests also transitions which contain
synchronisation. No faults are found in synchronisation, it can be assumed that
the synchronised transitions works, in the sense of the result of the tested
transitions were the same as the trace from the \gls{smt} solver.
Also, there does not seem to be any faults in partial failure or asynchrony.
As discussed before, the synchronised transitions are also translated to logical
formulas.
These formulas contain no logic about synchronisation, and therefore it should
be possible to identify misbehaviour in synchronisation. Several studies also
reports to find faults in distributed algorithms with \gls{smt}-based approaches
to model checking.~\cite{konnov2015you, alberti2015smt, mccaffrey2016verification}
