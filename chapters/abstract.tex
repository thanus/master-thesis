\abstract{
  Growing systems is a concern for large organisations. The continuity of
  systems becomes difficult, and a single modification can result in unexpected
  behaviour of a larger part of the system. Within the domain knowledge,
  reasoning about the expected behaviour of a system, changes and errors are
  hard. \textit{Rebel} aims to solve these challenges by centralising the domain
  knowledge and relating it to the running systems. \textit{Rebel} is a formal
  specification language in which financial products can be specified.

  Software testing is an important part of software projects. To facilitate the
  process \textit{Rebel} offers automated simulation and checking of
  specifications with the use of a satisfiability modulo theories solver. This
  solves to some extent the testing and reasoning of \textit{Rebel}
  specifications, but this is only within in the \textit{Rebel} domain.

  Code generators generate code from the \textit{Rebel} specifications.
  It is not always straightforward to generate a correct system from
  \textit{Rebel}. The problem with code generation is that the resulting product
  is leaving the \textit{Rebel} domain, causing loss of testing and reasoning
  with the use of formal methods. The running systems which are generated from
  specifications need to be properly based on these specifications; it should
  conform to these specifications.

  In this work, we have shown two proof of concepts to test generated systems
  from \textit{Rebel} specifications. With these proof of concepts, it can be
  tested whether the generated systems are generated properly based on
  \textit{Rebel} specifications. In both proofs of concepts, the satisfiability
  modulo theories solver holds the key in testing the generated systems. The
  result of this is that we regained the benefits from \textit{Rebel} domain,
  and again able to test and reason about \textit{Rebel} specifications and
  generated system.
  The generated systems are tested in two ways, invalid execution and valid
  execution. The first experiment tests invalid execution in the generated
  systems, \textit{i.e.}, testing what should be not possible according to the
  specification.
  The second experiment tests valid execution in the generated systems,
  \textit{i.e.}, testing what should be possible according to the specification.

  To sum up, with the experiments a total of five faults have been found in the
  generated system that is generated by the code generators.
  These faults can be categorised in the following categories: templating,
  compilation and distribution.
}
